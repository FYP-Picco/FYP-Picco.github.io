<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Project Webpage">
    <title>Sim-to-Real Transfer of VLN in Continuous Environments 
        Using an Ackermann-Steered Mobile Robot</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <!-- Header Section -->
    <header>
        <h1>Sim-to-Real Transfer of VLN in Continuous Environments 
            Using an Ackermann-Steered Mobile Robot</h1>
        <p>
            <span>Chalindu Abeywansa</span>, <span>Sahan Gunasekara</span>, <span>Devindi De Silva</span>, <span>Seniru Dissanayake</span> <br>
           <span>Ranga Rodrigo</span>, <span>Peshala Jayasekara</span>
        </p>
           
    </header>

    <!-- Contributors Section -->
    <!-- <section id="contributors">
        <h2>Contributors</h2>
        <ul>
            <li>Student 1</li>
            <li>Student 2</li>
            <li>Student 3</li>
            <li>Student 4</li>
            <li>Supervisor 1</li>
            <li>Supervisor 2</li>
        </ul>
    </section> -->

    <!-- Links Section -->
    <section id="links">
        <!-- <p>Vision-Language Navigation (VLN) enables robots to navigate through environments using natural language instructions, making human-robot interaction intuitive.</p> -->

        <h2>Project Links</h2>
        <div class="links">
            <a href="https://github.com/yourproject" target="_blank" class="button">GitHub Code</a>
            <a href="https://youtube.com/yourvideo" target="_blank" class="button">Project Video</a>
            <a href="https://researchpaperlink.com" target="_blank" class="button">Research Paper</a>
        </div>
    </section>

    <!-- 3-Minute Video Section -->
    <section id="video">
        <h2>Project Overview Video</h2>
        <iframe width="890" height="504" src="media/VIDEO Sim-to-Real Transfer of Vision-Language Navigation in Continuous Environments Using an Ackermann-Steered Mobile Robot.mp4" title="Project Video" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </section>

    <!-- Abstract Section -->
    <section id="abstract">
        <h2>Project Abstract</h2>
        <p>
        Vision-Language Navigation (VLN) enables robots
        to navigate through environments using natural language instructions, 
        making human-robot interaction intuitive. Traditional VLN models often 
        rely on navigation graphs, 360-degree
        views, and perfect localization which pose significant challenges
        when adapting these models to real-world settings. This work
        addresses these limitations by performing a simulation-to-real
        domain shift of a VLN approach that operates in continuous environments
        without requiring navigation graphs or panoramic
        views. The proposed system integrates vision-language models
        that align visual inputs and linguistic instructions within a
        shared embedding space, facilitating natural language-driven
        navigation. We employ a Cross-Modal Attention (CMA) based
        architecture trained on an existing dataset in a simulated
        environment and fine-tune it using real-world data collected
        from a custom-built Ackermann-steered robot equipped with
        a camera and a LiDAR sensor. By utilising linear photometric
        adjustments and fine-tuning on a limited number of episodes,
        our model successfully adapts to real-world environments,
        achieving effective navigation while running offline on dedicated 
        hardware. Experimental results, evaluated using Success
        weighted by Path Length (SPL) and Normalized Dynamic Time
        Warping (nDTW) metrics, demonstrate the robustness and
        adaptability of our approach.
        </p>
    </section>

    <!-- Approach Section -->
    <section id="approach">
        <h2>Project Approach</h2>
        <h3>System Architecture</h3>
        <!-- <p>Description of the system architecture, outlining key components and how they interact.</p> -->
        <img src="media/Full-Arch.png" alt="System Architecture" width="900">

        <h3>Experiments</h3>
        <p>Details about the experiments conducted, including methodology, datasets used, and evaluation metrics. Include key findings here as well.</p>
    </section>

    <!-- Contributors Images Section -->
    <section id="images">
        <h2>Contributors</h2>
        <div class="image-gallery">
            <img src="student1.jpg" alt="Chalindu">
            <img src="student2.jpg" alt="Sahan">
            <img src="student3.jpg" alt="Devindi">
            <img src="student4.jpg" alt="Seniru">
            <img src="supervisor1.jpg" alt="Dr. Ranga Rodrigo">
            <img src="supervisor2.jpg" alt="Dr. Peshala Jayasekara">
        </div>
    </section>

    <!-- Footer Section -->
    <footer>
        <p>&copy; 2024 Sim-to-Real Transfer of VLN in Continuous Environments 
            Using an Ackermann-Steered Mobile Robot. All rights reserved.
        </p>
    </footer>

    <script src="script.js"></script>
</body>
</html>
